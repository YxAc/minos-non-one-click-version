[configuration]
  # The raw files need to read when generating the configuration
  configuration.xsl=%{config_dir}/template/configuration.xsl
  log4j.xml=%{config_dir}/template/hdfs/log4j.xml
  krb5.conf=%{config_dir}/krb5-hadoop.conf
  rackinfo.txt=%{config_dir}/rackinfo.txt
  hadoop-groups.conf=%{config_dir}/hadoop-groups.conf
  excludes=%{config_dir}/excludes
  pre.sh=%{config_dir}/template/hdfs/pre.sh
  post.sh=%{config_dir}/template/hdfs/post.sh

  [[core-site.xml]]
    # The core-site.xml config content
    fs.defaultFS=hdfs://%{cluster.name}
    ha.zookeeper.quorum=%{zk.hosts_with_port}

    hadoop.http.staticuser.user=hdfs
    hadoop.tmp.dir=/tmp/hadoop
    io.file.buffer.size=131072

    # security authentication switch
    hadoop.security.authentication=simple
    hadoop.security.authorization=false
    hadoop.security.use-weak-http-crypto=false

    # Configure to enable hdfs users can access hdfs through the webhdfs proxy user 'hue'
    hadoop.proxyuser.hue.hosts=*
    hadoop.proxyuser.hue.groups=*
    hue.kerberos.principal.shortname=hue

  [[hdfs-site.xml]]
    # The hdfs-site.xml config content
    dfs.nameservices=%{cluster.name}
    dfs.ha.namenodes.%{cluster.name}="host0,host1"
    dfs.client.failover.proxy.provider.%{cluster.name}=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider
    dfs.namenode.rpc-address.%{cluster.name}.host0=%{namenode.0.host}:%{namenode.0.base_port}
    dfs.namenode.rpc-address.%{cluster.name}.host1=%{namenode.1.host}:%{namenode.1.base_port}
    dfs.namenode.http-address.%{cluster.name}.host0=%{namenode.0.host}:%{namenode.0.base_port+1}
    dfs.namenode.http-address.%{cluster.name}.host1=%{namenode.1.host}:%{namenode.1.base_port+1}

    dfs.journalnode.rpc-address=0.0.0.0:%{journalnode.base_port}
    dfs.journalnode.http-address=0.0.0.0:%{journalnode.base_port+1}
    dfs.journalnode.edits.dir=%{journalnode.data_dir}

    dfs.namenode.shared.edits.dir=qjournal://%{journalnode_task_list}/%{cluster.name}
    dfs.ha.zkfc.port=%{zkfc.base_port}
    dfs.namenode.name.dir=%{namenode.data_dir}
    net.topology.table.file.name=%{namenode.run_dir}/rackinfo.txt
    net.topology.node.switch.mapping.impl=org.apache.hadoop.net.TableMapping
    dfs.hosts.exclude=%{namenode.run_dir}/excludes
    hadoop.security.group.mapping.file.name=%{namenode.run_dir}/hadoop-groups.conf

    dfs.block.local-path-access.user="work, hbase, hbase_srv, impala"
    dfs.datanode.ipc.address=0.0.0.0:%{datanode.base_port}
    dfs.datanode.http.address=0.0.0.0:%{datanode.base_port+1}
    dfs.datanode.address=0.0.0.0:%{datanode.base_port+2}
    dfs.datanode.data.dir=%{datanode.data_dirs}

    dfs.client.read.shortcircuit.skip.auth=true

    dfs.datanode.balance.bandwidthPerSec=10485760
    dfs.namenode.replication.min=1
    dfs.namenode.safemode.threshold-pct=0.99f

    dfs.ha.fencing.methods="sshfence&#xA;shell(/bin/true)"
    dfs.ha.fencing.ssh.private-key-files=/home/work/.ssh/id_rsa
    dfs.ha.fencing.ssh.connect-timeout=2000
    dfs.ha.automatic-failover.enabled=true

    dfs.datanode.max.xcievers=4096
    dfs.namenode.handler.count=64
    dfs.block.size=128m
    dfs.client.read.shortcircuit=true
    fs.trash.interval=10080
    fs.trash.checkpoint.interval=1440

    # config for dfs acl
    dfs.web.ugi="hdfs,supergroup"
    dfs.permissions.superusergroup=supergroup
    dfs.permissions.superuser=hdfs_admin
    dfs.namenode.upgrade.permission=0777
    fs.permissions.umask-mode=022
    dfs.cluster.administrators=hdfs_admin
    dfs.permissions.enabled=false
    # hadoop.security.group.mapping=org.apache.hadoop.security.ConfigurationBasedGroupsMapping

    # config security
    dfs.block.access.token.enable=true
    ignore.secure.ports.for.testing=true
    # namenode security config
    dfs.namenode.kerberos.internal.spnego.principal=HTTP/hadoop@EXAMPLE.HADOOP
    dfs.namenode.keytab.file=/etc/hadoop/conf/hdfs.keytab
    dfs.namenode.kerberos.principal=hdfs/hadoop@EXAMPLE.HADOOP
    # secondary namenode security config
    dfs.secondary.namenode.kerberos.internal.spnego.principal=HTTP/hadoop@EXAMPLE.HADOOP
    dfs.secondary.namenode.keytab.file=/etc/hadoop/conf/hdfs.keytab
    dfs.secondary.namenode.kerberos.principal=hdfs/hadoop@EXAMPLE.HADOOP
    # datanode security config
    dfs.datanode.data.dir.perm=700
    dfs.datanode.keytab.file=/etc/hadoop/conf/hdfs.keytab
    dfs.datanode.kerberos.principal=hdfs/hadoop@EXAMPLE.HADOOP
    # journalnode security config
    dfs.journalnode.kerberos.internal.spnego.principal=HTTP/hadoop@EXAMPLE.HADOOP
    dfs.journalnode.keytab.file=/etc/hadoop/conf/hdfs.keytab
    dfs.journalnode.kerberos.principal=hdfs/hadoop@EXAMPLE.HADOOP
    # config web
    dfs.web.authentication.kerberos.principal=HTTP/hadoop@EXAMPLE.HADOOP
    dfs.web.authentication.kerberos.keytab=/etc/hadoop/conf/hdfs.keytab

[arguments]

  [[service_common]]
    jvm_args='''
      -Xmx1024m
      -Xms1024m
      -Xmn512m
      -XX:MaxDirectMemorySize=1024m
      -XX:MaxPermSize=128m
      -XX:+DisableExplicitGC
      -XX:+HeapDumpOnOutOfMemoryError
      -XX:HeapDumpPath=$log_dir
      -XX:+PrintGCApplicationStoppedTime
      -XX:+UseConcMarkSweepGC
      -XX:CMSInitiatingOccupancyFraction=80
      -XX:+UseMembar
      -verbose:gc
      -XX:+PrintGCDetails
      -XX:+PrintGCDateStamps
    '''

    system_properties='''
      -Djava.net.preferIPv4Stack=true
      -Dhdfs.log.dir=$log_dir
      -Dhdfs.pid=$pid
      -Dhdfs.cluster=%{cluster.name}
      -Dhdfs.log.level=%{cluster.log_level}
      -Dhadoop.policy.file=hadoop-policy.xml
      -Dhadoop.home.dir=$package_dir
      -Djava.security.krb5.conf=$run_dir/krb5.conf
      -Dhadoop.id.str=%{remote_user}
      -Dkerberos.instance=hadoop
    '''

    main_entry='''
    '''

    extra_args='''
    '''

  [[journalnode]]
    jvm_args='''
      -Xloggc:$run_dir/stdout/journalnode_gc_${start_time}.log
    '''
    system_properties='''
      -Dproc_journalnode
    '''
    main_entry='''
      org.apache.hadoop.hdfs.qjournal.server.JournalNode
    '''

  [[namenode]]
    jvm_args='''
      -Xloggc:$run_dir/stdout/namenode_gc_${start_time}.log
    '''
    system_properties='''
      -Dproc_namenode
    '''
    main_entry='''
      org.apache.hadoop.hdfs.server.namenode.NameNode
    '''

  [[zkfc]]
    jvm_args='''
      -Xloggc:$run_dir/stdout/zkfc_gc_${start_time}.log
    '''
    system_properties='''
      -Dproc_zkfc
    '''
    main_entry='''
      org.apache.hadoop.hdfs.tools.DFSZKFailoverController
    '''

  [[datanode]]
    jvm_args='''
      -Xloggc:$run_dir/stdout/datanode_gc_${start_time}.log
    '''
    system_properties='''
      -Dproc_datanode
    '''
    main_entry='''
      org.apache.hadoop.hdfs.server.datanode.DataNode
    '''
